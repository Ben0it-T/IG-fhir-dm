# HAPI FHIR Transaction Uploader

## Objective
Create a Python script to upload FHIR Bundle JSON files to HAPI FHIR server using transaction bundles, respecting referential integrity order based on `upload-plan.json` generated by `prepare_fhir_bundles.py`.

## Project Structure

```
data-platform/
└── share-layer/
      └── minio_to_fhir/
          ├── .env.example              # Configuration template
          ├── .env                      # Local config (gitignored)
          ├── .gitignore                # Git ignore rules
          ├── requirements.txt          # Python dependencies
          ├── README.md                 # Project documentation
          ├── common/                   # Shared utilities
          │   ├── __init__.py
          │   ├── config.py             # Configuration loader
          │   ├── minio_client.py       # MinIO client wrapper
          │   ├── fhir_client.py        # FHIR client wrapper
          │   └── utils.py              # Common utilities
          ├── list_minio_resources.py     # Feature 1: List
          ├── download_minio_resources.py # Feature 2: Download
          ├── prepare_fhir_bundles.py     # Feature 3: Prepare bundles
          └── upload_to_fhir.py           # Feature 4: Upload (this script)
```

## Configuration (.env)
```bash
# HAPI FHIR Configuration
FHIR_BASE_URL=http://localhost:8080/fhir
FHIR_AUTH_ENABLED=false
FHIR_USERNAME=
FHIR_PASSWORD=

# Upload Configuration
TMP_DIR=/tmp/fhir-upload
BATCH_SIZE=100
```

## Command-Line Interface
```bash
# Basic usage - reads upload-plan.json from input directory
python upload_to_fhir.py --input ./tmp/fhir-upload

# With custom upload plan location
python upload_to_fhir.py --input ./tmp/fhir-upload --plan-file ./custom-plan.json

# Help
python upload_to_fhir.py --help
```

**Arguments**:
- `--input` / `-i`: Source directory containing upload-plan.json and level-XX/ directories (overrides .env)
- `--plan-file` / `-p`: Path to upload-plan.json (default: `<input>/upload-plan.json`)

## Upload Workflow

Reads `upload-plan.json` generated by `prepare_fhir_bundles.py`:

**Input Structure**:
```
tmp/fhir-upload/
├── upload-plan.json              # Dependency analysis & upload order
├── level-01/                     # Foundation resources
│   ├── medication-batch-001.json
│   └── organization-batch-001.json
├── level-02/                     # Patient context
│   ├── location-batch-001.json
│   └── patient-batch-001.json
├── level-03/                     # Encounters
│   ├── encounter-batch-001.json
│   └── specimen-batch-001.json
└── level-XX/                     # Additional levels
    └── ...
```

**upload-plan.json Structure**:
```json
{
  "analysis_timestamp": "2025-10-09T21:38:12.447605",
  "total_resources": 928935,
  "total_bundles": 937,
  "batch_size": 1000,
  "reference_analysis_enabled": true,
  "dependency_levels": [
    {
      "level": 1,
      "name": "Foundation Resources",
      "resource_types": ["Medication", "Organization"],
      "total_resources": 1795,
      "bundles": 3,
      "dependencies": []
    }
  ],
  "circular_references": [["Encounter"], ["Observation"]],
  "circular_reference_instances": [
    {
      "resource_type": "Observation",
      "resource_a": "Observation/abc",
      "resource_b": "Observation/xyz",
      "description": "..."
    }
  ],
  "orphaned_references": []
}
```

**Upload Process**:
1. Load and validate upload-plan.json
2. Display warnings (circular refs, orphaned refs)
3. Verify all level directories exist
4. Process levels sequentially: level-01 → level-02 → level-03 → ...
5. Within each level, upload all bundles sequentially
6. Track statistics per level and overall
7. Log all operations to file

## Core Functionality

1. **Load configuration** from `.env` file
2. **Parse command-line arguments**
3. **Test HAPI FHIR server** connectivity
4. **Load upload-plan.json**:
   - Validate structure
   - Extract dependency levels
   - Display warnings (circular refs, orphaned refs)
5. **Scan level directories** (level-01, level-02, etc.)
6. **Upload by level** (sequential):
   - For each level directory:
     - Find all bundle JSON files
     - Upload each bundle to FHIR server
     - Parse response (count successes/failures)
     - Log results
   - Display level statistics
7. **Display final summary**
8. **Exit with appropriate code** (0 = success, 1 = failures)

## Output Format
```
=== HAPI FHIR Batch Uploader ===
Mode: Plan-Based Upload
Source: ./tmp/fhir-upload
Plan: ./tmp/fhir-upload/upload-plan.json
Destination: http://localhost:8080/fhir
Batch Size: 1000 (from plan)

Testing FHIR server... OK (HAPI FHIR 6.8.0)

Loading upload plan...
  Analysis timestamp: 2025-10-09T21:38:12
  Total bundles: 937 across 5 dependency levels
  Total resources: 928,935

WARNINGS:
  - 10 mutual references detected in Observation resources
  - See upload-plan.json for details

Upload order:
  Level 1: Medication, Organization (3 bundles, 1,795 resources)
  Level 2: Location, Patient (2 bundles, 131 resources)
  Level 3: Encounter, Specimen (14 bundles, 13,095 resources)
  Level 4: Condition, MedicationRequest, MedicationStatement, Procedure (31 bundles, 28,464 resources)
  Level 5: MedicationAdministration, MedicationDispense, Observation (887 bundles, 885,450 resources)

Starting upload...

[Level 1/5] Foundation Resources (3 bundles)
  medication-batch-001.json ... OK (1000 created, 0 failed) - 2.3s
  medication-batch-002.json ... OK (794 created, 0 failed) - 1.9s
  organization-batch-001.json ... OK (1 created, 0 failed) - 0.2s
  Level 1 complete: 1,795 resources uploaded

[Level 2/5] Patient Context (2 bundles)
  location-batch-001.json ... OK (31 created, 0 failed) - 0.3s
  patient-batch-001.json ... OK (100 created, 0 failed) - 1.1s
  Level 2 complete: 131 resources uploaded

[Level 3/5] Encounters (14 bundles)
  encounter-batch-001.json ... OK (1000 created, 0 failed) - 5.2s
  ...
  specimen-batch-013.json ... OK (458 created, 0 failed) - 2.1s
  Level 3 complete: 13,095 resources uploaded

[Level 4/5] Clinical Context (31 bundles)
  ...
  Level 4 complete: 28,464 resources uploaded

[Level 5/5] Clinical Data (887 bundles)
  ...
  Level 5 complete: 885,450 resources uploaded

=== Summary ===
Total bundles: 937
Total resources: 928,935
Successfully uploaded: 928,800
Failed: 135
Duration: 15m 23s

Failed resources logged to: fhir_upload.log
```

## Dependencies (requirements.txt)
```txt
minio>=7.2.0
python-dotenv>=1.0.0
requests>=2.31.0
```

## Error Handling
- Test FHIR server before upload
- Handle batch failures gracefully
- Track failed resources
- Log failures to file
- Retry logic for network errors
- Clear error messages

## Code Structure

```python
#!/usr/bin/env python3
"""
HAPI FHIR Transaction Uploader
Upload FHIR Bundle JSON files to HAPI FHIR server using transaction bundles.
Respects referential integrity by uploading resources in dependency order.
"""

import json
import argparse
import time
from pathlib import Path
from dotenv import load_dotenv
import requests
from typing import List, Dict, Tuple

# Resource loading order (referential integrity)
RESOURCE_ORDER = [
    'Organization',
    'Location',
    'Patient',
    'Encounter',
    'Condition',
    'Specimen',
    'Observation',
    'Procedure',
    'Medication',
    'MedicationRequest',
    'MedicationDispense',
    ['MedicationStatement', 'MedicationAdministration']
]

def parse_arguments():
    """Parse command-line arguments."""
    pass

def load_config(input_override=None):
    """Load environment variables."""
    pass

def test_fhir_server(fhir_url, auth=None):
    """Test HAPI FHIR server connectivity."""
    # GET /metadata
    pass

def get_resource_type_from_filename(filename):
    """Extract FHIR resource type from filename."""
    # Handle patterns like:
    # - "Patient.ndjson" -> "Patient"
    # - "MimicPatient.ndjson" -> "Patient"
    # - "Patient_part1.ndjson" -> "Patient"
    pass

def scan_ndjson_files(directory):
    """Scan directory for NDJSON files and group by resource type."""
    pass

def sort_files_by_order(files_by_type):
    """Sort files according to RESOURCE_ORDER."""
    pass

def read_ndjson_file(filepath):
    """Read NDJSON file and yield resources."""
    pass

def create_transaction_bundle(resources: List[Dict]) -> Dict:
    """Create FHIR Bundle of type 'transaction'."""
    pass

def upload_bundle(fhir_url: str, bundle: Dict, auth=None) -> Tuple[int, int]:
    """Upload transaction bundle to FHIR server."""
    # POST bundle
    # Parse response
    # Return (success_count, failure_count)
    pass

def upload_file(filepath, fhir_url, batch_size, auth=None):
    """Upload all resources from a single NDJSON file."""
    # Read file
    # Create batches
    # Upload each batch
    # Return stats
    pass

def upload_all_files(sorted_files, fhir_url, batch_size, auth=None):
    """Upload all files in correct order."""
    pass

def log_failed_resource(resource, error, log_file):
    """Log failed resource to file."""
    pass

def main():
    """Main entry point."""
    pass

if __name__ == "__main__":
    main()
```

## Success Criteria
✅ Tests FHIR server connectivity  
✅ Reads NDJSON files correctly  
✅ Groups files by resource type
✅ Sorts files by dependency order  
✅ Creates valid FHIR batch bundles  
✅ Uploads batches successfully  
✅ Tracks success/failure per resource  
✅ Logs failures for review  
✅ Handles errors gracefully  
✅ Displays clear progress  
✅ Respects referential integrity order