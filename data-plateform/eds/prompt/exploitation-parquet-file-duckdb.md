# DuckDB Parquet File Exploitation Guide

## Overview
DuckDB provides powerful capabilities for working with Parquet files, offering high-performance, columnar analytics directly on Parquet datasets.

## Installation
```bash
pip install duckdb
```

## Basic Querying

### Direct Parquet File Querying
```python
import duckdb

# Query a single Parquet file
duckdb.sql("SELECT * FROM 'data.parquet'")

# Query multiple Parquet files
duckdb.sql("SELECT * FROM 'data/*.parquet'")
```

## Performance Optimization Techniques

### 1. Row Group Optimization
```python
# When writing Parquet files, set optimal row group size
duckdb.sql("""
    COPY (FROM generate_series(100_000)) 
    TO 'test.parquet' (
        FORMAT parquet, 
        ROW_GROUP_SIZE 100_000
    )
""")
```

### 2. Projection and Filter Pushdown
DuckDB automatically optimizes queries by:
- Reading only required columns
- Skipping unnecessary data using zonemaps
- Applying filters directly to Parquet file scans

### 3. Handling Multiple Files
```python
# Read multiple files with flexible schema
duckdb.sql("""
    SELECT * FROM read_parquet(
        ['file1.parquet', 'file2.parquet'], 
        union_by_name = true
    )
""")
```

### 4. Parallel Writing
```python
# Write Parquet files in parallel
duckdb.sql("""
    COPY (FROM generate_series(10_000_000)) 
    TO 'test.parquet' (
        FORMAT parquet, 
        PER_THREAD_OUTPUT
    )
""")
```

## Connection Management Best Practices
```python
# Recommended for packages and complex workflows
with duckdb.connect() as con:
    result = con.sql("SELECT * FROM 'data.parquet'")
```

## DuckDB 1.3 Performance Features

### Late Materialization
- Defers column fetching until necessary
- 3-10x faster reads for queries with LIMIT

### Compression Strategies
- Prefer lightweight compression (Snappy, LZ4, zstd)
- Avoid heavyweight compression like gzip

### Remote Data Caching
- Caches remote Parquet files
- Improves performance for repeated remote file access

## Recommendations

1. Use DuckDB for analytical workloads with Parquet files
2. Prefer direct querying for occasional use
3. Load into database for repeated, complex queries
4. Optimize row group sizes (100K-1M rows)
5. Leverage automatic optimizations

## Example Analytics Workflow
```python
import duckdb
import pandas as pd

# Connect to DuckDB
con = duckdb.connect()

# Read Parquet files
df = con.sql("""
    SELECT 
        patient_id, 
        AVG(measurement_value) as avg_measurement
    FROM 'patient_data/*.parquet'
    WHERE measurement_date > '2023-01-01'
    GROUP BY patient_id
    ORDER BY avg_measurement DESC
    LIMIT 100
""").df()

# Further pandas processing if needed
print(df)
```

## Performance Tips
- Use `EXPLAIN` to understand query plans
- Monitor memory usage
- Leverage built-in optimizations
- Consider loading frequently queried datasets

## Potential Gotchas
- Check Parquet file row group sizes
- Be mindful of compression methods
- Use context managers for connection handling